[2024-01-07 17:45:05,224] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-07 17:45:07,930] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2024-01-07 17:45:07,930] [INFO] [runner.py:555:main] cmd = /home/lfsm/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/lfsm/code/Robin//robin/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path EleutherAI/pythia-410m --version plain --data_path /home/lfsm/code/Robin/playground-original/llava_pretrain/blip_laion_cc_sbu_558k.json --image_folder /home/lfsm/code/Robin/playground-original/llava_pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --finetune_ve False --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --fp16 True --output_dir /home/lfsm/code/Robin/robin/checkpoints/pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing False --dataloader_num_workers 4 --lazy_preprocess True
[2024-01-07 17:45:09,209] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-07 17:45:11,718] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-01-07 17:45:11,718] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-01-07 17:45:11,718] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-01-07 17:45:11,718] [INFO] [launch.py:163:main] dist_world_size=2
[2024-01-07 17:45:11,718] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-01-07 17:45:14,859] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-07 17:45:14,863] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-07 17:45:16,541] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-01-07 17:45:16,541] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-01-07 17:45:16,541] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-01-07 17:45:16,541] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-01-07 17:45:16,541] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
ModelArguments(model_name_or_path='EleutherAI/pythia-410m', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_vision_select_feature='patch')
ModelArguments(model_name_or_path='EleutherAI/pythia-410m', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_vision_select_feature='patch')
Formatting inputs...Skip in lazy mode
LlavaGPTNeoXForCausalLM(
  (gpt_neox): LlavaGPTNeoXModel(
    (embed_in): Embedding(50304, 1024)
    (emb_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-23): 24 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (post_attention_dropout): Dropout(p=0.0, inplace=False)
        (post_mlp_dropout): Dropout(p=0.0, inplace=False)
        (attention): GPTNeoXAttention(
          (rotary_emb): GPTNeoXRotaryEmbedding()
          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)
          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)
)
Rank: 1 partition count [2, 2] and sizes[(1048576, False), (1024, False)] 
Rank: 0 partition count [2, 2] and sizes[(1048576, False), (1024, False)] 
{'loss': 7.5859, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 7.4512, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 7.4219, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 7.4668, 'learning_rate': 3.816793893129771e-06, 'epoch': 0.0}
{'loss': 7.7051, 'learning_rate': 7.633587786259541e-06, 'epoch': 0.0}
{'loss': 7.6035, 'learning_rate': 1.1450381679389314e-05, 'epoch': 0.0}
{'loss': 7.252, 'learning_rate': 1.1450381679389314e-05, 'epoch': 0.0}
{'loss': 7.4199, 'learning_rate': 1.5267175572519083e-05, 'epoch': 0.0}
{'loss': 7.2754, 'learning_rate': 1.9083969465648855e-05, 'epoch': 0.0}
{'loss': 7.4746, 'learning_rate': 2.2900763358778628e-05, 'epoch': 0.0}
{'loss': 7.043, 'learning_rate': 2.6717557251908397e-05, 'epoch': 0.0}
{'loss': 7.0898, 'learning_rate': 3.0534351145038166e-05, 'epoch': 0.0}
{'loss': 7.0801, 'learning_rate': 3.435114503816794e-05, 'epoch': 0.0}
{'loss': 6.4941, 'learning_rate': 3.816793893129771e-05, 'epoch': 0.0}
{'loss': 6.6426, 'learning_rate': 4.198473282442748e-05, 'epoch': 0.0}
{'loss': 6.543, 'learning_rate': 4.5801526717557256e-05, 'epoch': 0.0}
{'loss': 6.5801, 'learning_rate': 4.9618320610687025e-05, 'epoch': 0.0}
{'loss': 6.7246, 'learning_rate': 5.3435114503816794e-05, 'epoch': 0.0}
{'loss': 6.4609, 'learning_rate': 5.725190839694656e-05, 'epoch': 0.0}
{'loss': 6.3926, 'learning_rate': 6.106870229007633e-05, 'epoch': 0.0}
{'loss': 6.3008, 'learning_rate': 6.488549618320611e-05, 'epoch': 0.0}
{'loss': 6.3223, 'learning_rate': 6.870229007633588e-05, 'epoch': 0.0}
{'loss': 6.2383, 'learning_rate': 7.251908396946565e-05, 'epoch': 0.0}
{'loss': 5.8535, 'learning_rate': 7.633587786259542e-05, 'epoch': 0.0}
{'loss': 5.9766, 'learning_rate': 8.015267175572518e-05, 'epoch': 0.0}
{'loss': 5.9805, 'learning_rate': 8.396946564885496e-05, 'epoch': 0.0}
{'loss': 6.1367, 'learning_rate': 8.778625954198472e-05, 'epoch': 0.0}
{'loss': 6.0039, 'learning_rate': 9.160305343511451e-05, 'epoch': 0.0}
{'loss': 5.8262, 'learning_rate': 9.541984732824429e-05, 'epoch': 0.0}
{'loss': 5.8359, 'learning_rate': 9.923664122137405e-05, 'epoch': 0.0}
{'loss': 5.8984, 'learning_rate': 0.00010305343511450383, 'epoch': 0.0}
{'loss': 5.9121, 'learning_rate': 0.00010687022900763359, 'epoch': 0.0}
{'loss': 5.9883, 'learning_rate': 0.00011068702290076336, 'epoch': 0.0}
{'loss': 5.5684, 'learning_rate': 0.00011450381679389313, 'epoch': 0.0}
{'loss': 5.6367, 'learning_rate': 0.0001183206106870229, 'epoch': 0.0}
{'loss': 5.8906, 'learning_rate': 0.00012213740458015266, 'epoch': 0.0}
{'loss': 5.541, 'learning_rate': 0.00012595419847328244, 'epoch': 0.0}
{'loss': 5.7383, 'learning_rate': 0.00012977099236641222, 'epoch': 0.0}
{'loss': 5.4785, 'learning_rate': 0.000133587786259542, 'epoch': 0.0}
{'loss': 5.9375, 'learning_rate': 0.00013740458015267177, 'epoch': 0.0}
{'loss': 5.4922, 'learning_rate': 0.00014122137404580154, 'epoch': 0.0}
{'loss': 5.5957, 'learning_rate': 0.0001450381679389313, 'epoch': 0.0}
{'loss': 5.375, 'learning_rate': 0.00014885496183206107, 'epoch': 0.0}
{'loss': 5.625, 'learning_rate': 0.00015267175572519084, 'epoch': 0.01}
{'loss': 5.4316, 'learning_rate': 0.00015648854961832062, 'epoch': 0.01}
{'loss': 5.3613, 'learning_rate': 0.00016030534351145037, 'epoch': 0.01}
{'loss': 5.3008, 'learning_rate': 0.00016412213740458014, 'epoch': 0.01}
{'loss': 5.2715, 'learning_rate': 0.00016793893129770992, 'epoch': 0.01}
{'loss': 5.3379, 'learning_rate': 0.0001717557251908397, 'epoch': 0.01}
{'loss': 5.5723, 'learning_rate': 0.00017557251908396944, 'epoch': 0.01}
{'loss': 5.3398, 'learning_rate': 0.00017938931297709925, 'epoch': 0.01}
{'loss': 5.3027, 'learning_rate': 0.00018320610687022902, 'epoch': 0.01}
{'loss': 5.3164, 'learning_rate': 0.0001870229007633588, 'epoch': 0.01}
{'loss': 5.3145, 'learning_rate': 0.00019083969465648857, 'epoch': 0.01}
{'loss': 5.1719, 'learning_rate': 0.00019465648854961832, 'epoch': 0.01}
{'loss': 5.3398, 'learning_rate': 0.0001984732824427481, 'epoch': 0.01}
{'loss': 5.3438, 'learning_rate': 0.00020229007633587788, 'epoch': 0.01}
{'loss': 5.1465, 'learning_rate': 0.00020610687022900765, 'epoch': 0.01}
{'loss': 5.3574, 'learning_rate': 0.0002099236641221374, 'epoch': 0.01}
{'loss': 5.1562, 'learning_rate': 0.00021374045801526718, 'epoch': 0.01}
{'loss': 4.9629, 'learning_rate': 0.00021755725190839695, 'epoch': 0.01}
{'loss': 5.0723, 'learning_rate': 0.00022137404580152673, 'epoch': 0.01}
{'loss': 4.9688, 'learning_rate': 0.00022519083969465648, 'epoch': 0.01}
{'loss': 5.0742, 'learning_rate': 0.00022900763358778625, 'epoch': 0.01}
{'loss': 5.1289, 'learning_rate': 0.00023282442748091603, 'epoch': 0.01}
{'loss': 5.3867, 'learning_rate': 0.0002366412213740458, 'epoch': 0.01}
{'loss': 5.1641, 'learning_rate': 0.00024045801526717558, 'epoch': 0.01}
{'loss': 5.1016, 'learning_rate': 0.00024427480916030533, 'epoch': 0.01}
{'loss': 5.123, 'learning_rate': 0.00024809160305343513, 'epoch': 0.01}
{'loss': 4.7285, 'learning_rate': 0.0002519083969465649, 'epoch': 0.01}
{'loss': 5.1562, 'learning_rate': 0.00025572519083969463, 'epoch': 0.01}
{'loss': 5.2285, 'learning_rate': 0.00025954198473282443, 'epoch': 0.01}
{'loss': 4.9883, 'learning_rate': 0.0002633587786259542, 'epoch': 0.01}
{'loss': 4.9883, 'learning_rate': 0.000267175572519084, 'epoch': 0.01}
{'loss': 5.0488, 'learning_rate': 0.00027099236641221373, 'epoch': 0.01}
{'loss': 4.8301, 'learning_rate': 0.00027480916030534353, 'epoch': 0.01}
{'loss': 5.1191, 'learning_rate': 0.0002786259541984733, 'epoch': 0.01}
{'loss': 4.834, 'learning_rate': 0.0002824427480916031, 'epoch': 0.01}
{'loss': 4.7598, 'learning_rate': 0.0002862595419847328, 'epoch': 0.01}
{'loss': 4.8281, 'learning_rate': 0.0002900763358778626, 'epoch': 0.01}
{'loss': 5.0449, 'learning_rate': 0.0002938931297709924, 'epoch': 0.01}
{'loss': 4.9062, 'learning_rate': 0.00029770992366412214, 'epoch': 0.01}
{'loss': 4.9531, 'learning_rate': 0.00030152671755725194, 'epoch': 0.01}
{'loss': 4.8477, 'learning_rate': 0.0003053435114503817, 'epoch': 0.01}
{'loss': 4.75, 'learning_rate': 0.0003091603053435115, 'epoch': 0.01}
{'loss': 5.0859, 'learning_rate': 0.00031297709923664124, 'epoch': 0.01}
{'loss': 4.8926, 'learning_rate': 0.000316793893129771, 'epoch': 0.01}
{'loss': 4.6113, 'learning_rate': 0.00032061068702290074, 'epoch': 0.01}
{'loss': 4.8867, 'learning_rate': 0.00032442748091603054, 'epoch': 0.01}
{'loss': 4.7676, 'learning_rate': 0.0003282442748091603, 'epoch': 0.01}
{'loss': 4.8086, 'learning_rate': 0.0003320610687022901, 'epoch': 0.01}
{'loss': 4.5547, 'learning_rate': 0.00033587786259541984, 'epoch': 0.01}
{'loss': 5.0684, 'learning_rate': 0.00033969465648854964, 'epoch': 0.01}
{'loss': 4.9688, 'learning_rate': 0.0003435114503816794, 'epoch': 0.01}
{'loss': 4.7578, 'learning_rate': 0.0003473282442748092, 'epoch': 0.01}
{'loss': 4.7852, 'learning_rate': 0.0003511450381679389, 'epoch': 0.01}
{'loss': 4.8984, 'learning_rate': 0.0003549618320610687, 'epoch': 0.01}
{'loss': 4.6875, 'learning_rate': 0.0003587786259541985, 'epoch': 0.01}
{'loss': 4.6992, 'learning_rate': 0.00036259541984732824, 'epoch': 0.01}
[2024-01-07 17:47:33,217] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3614764
[2024-01-07 17:47:33,509] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3614765
[2024-01-07 17:47:33,815] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting
