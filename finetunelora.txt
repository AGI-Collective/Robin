[2023-11-09 05:40:19,463] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-09 05:40:22,150] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-09 05:40:22,150] [INFO] [runner.py:555:main] cmd = /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/envs/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/hf/vicuna-7b --version v1 --data_path /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/datasets/playground/data/llava_v1_5_mix665k.json --image_folder /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/datasets/playground/data --vision_tower /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/hf/clip --pretrain_mm_mlp_adapter /pfss/mlde/workspaces/mlde_wsp_Ramstedt_Mila/daniel/robin_llava/checkpoints/llava-v1.5-7b-pretrain/checkpoint-300/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-lora2 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --finetune_ve True
